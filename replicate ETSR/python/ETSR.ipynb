{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textdistance\n",
    "import timeit\n",
    "import pandas as pds\n",
    "from sklearn import mixture\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = {'family name':'jaro-winkler','gender':'strict','country':'strict','birth year':'large'}\n",
    "covariates = ['X1','X2','X3','X4','X5']\n",
    "\n",
    "name_DF = 'DF_associations_N=73_2023-01-04.csv'\n",
    "DF = pds.read_csv(os.path.join('..','..','simulate data','datasets', name_DF), delimiter = ',')\n",
    "\n",
    "overlap = DF.sample(n = 15)\n",
    "\n",
    "A = pds.concat([DF.sample(n = 45), overlap]).drop(['Y'], axis = 1)\n",
    "A = A.reset_index(drop=True)\n",
    "\n",
    "B = pds.concat([DF.sample(n = 35), overlap])[list(identifiers.keys())+['Y']]\n",
    "B = B.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>family name</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>birth year</th>\n",
       "      <th>treatment</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nina</td>\n",
       "      <td>Yashina</td>\n",
       "      <td>F</td>\n",
       "      <td>RU</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>8.089946</td>\n",
       "      <td>4.279396</td>\n",
       "      <td>8.839126</td>\n",
       "      <td>6.761503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dirk-Bijan</td>\n",
       "      <td>Zarrinnam</td>\n",
       "      <td>M</td>\n",
       "      <td>DE</td>\n",
       "      <td>1981</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>8.876769</td>\n",
       "      <td>2.346136</td>\n",
       "      <td>8.148365</td>\n",
       "      <td>4.681018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle</td>\n",
       "      <td>Brigden</td>\n",
       "      <td>F</td>\n",
       "      <td>GB</td>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>8.133766</td>\n",
       "      <td>1.856616</td>\n",
       "      <td>8.478746</td>\n",
       "      <td>5.266938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christin</td>\n",
       "      <td>Müller</td>\n",
       "      <td>F</td>\n",
       "      <td>DE</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>8.746727</td>\n",
       "      <td>1.668986</td>\n",
       "      <td>8.795278</td>\n",
       "      <td>7.700038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabolik</td>\n",
       "      <td>Preti</td>\n",
       "      <td>F</td>\n",
       "      <td>IT</td>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>8.231791</td>\n",
       "      <td>0.500859</td>\n",
       "      <td>9.250222</td>\n",
       "      <td>5.189169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name family name gender country  birth year  treatment  X1        X2  \\\n",
       "0        Nina     Yashina      F      RU        1960          0  60  8.089946   \n",
       "1  Dirk-Bijan   Zarrinnam      M      DE        1981          0  39  8.876769   \n",
       "2    Michelle     Brigden      F      GB        1989          0  31  8.133766   \n",
       "3    Christin      Müller      F      DE        1965          0  55  8.746727   \n",
       "4    Diabolik       Preti      F      IT        1989          0  31  8.231791   \n",
       "\n",
       "         X3        X4        X5  \n",
       "0  4.279396  8.839126  6.761503  \n",
       "1  2.346136  8.148365  4.681018  \n",
       "2  1.856616  8.478746  5.266938  \n",
       "3  1.668986  8.795278  7.700038  \n",
       "4  0.500859  9.250222  5.189169  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family name</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>birth year</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raj</td>\n",
       "      <td>M</td>\n",
       "      <td>FR</td>\n",
       "      <td>1979</td>\n",
       "      <td>70.794479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ghinassi</td>\n",
       "      <td>M</td>\n",
       "      <td>IT</td>\n",
       "      <td>1945</td>\n",
       "      <td>34.448177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perez Simon</td>\n",
       "      <td>M</td>\n",
       "      <td>ES</td>\n",
       "      <td>1987</td>\n",
       "      <td>78.687278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preti</td>\n",
       "      <td>F</td>\n",
       "      <td>IT</td>\n",
       "      <td>1989</td>\n",
       "      <td>7.369748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zaworski</td>\n",
       "      <td>M</td>\n",
       "      <td>PL</td>\n",
       "      <td>2019</td>\n",
       "      <td>-10.102921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   family name gender country  birth year          Y\n",
       "0          Raj      M      FR        1979  70.794479\n",
       "1     Ghinassi      M      IT        1945  34.448177\n",
       "2  Perez Simon      M      ES        1987  78.687278\n",
       "3        Preti      F      IT        1989   7.369748\n",
       "4     Zaworski      M      PL        2019 -10.102921"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_similarity(a,b):\n",
    "\n",
    "    \"\"\" Check that levenshtein similarity (in [0,1]) is above 0.95.\n",
    "        \n",
    "        a: string,\n",
    "        b: string \"\"\"\n",
    "\n",
    "    if 1 - textdistance.levenshtein(a, b)/max(len(a),len(b)) >= 0.95:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def jaro_winkler_similarity(a,b):\n",
    "\n",
    "    \"\"\" Check that jaro-winkler similarity (in [0,1]) is above 0.95.\n",
    "        \n",
    "        a: string,\n",
    "        b: string \"\"\"\n",
    "\n",
    "    if textdistance.jaro_winkler(a,b) >= 0.99:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def strict_equality(a,b):\n",
    "\n",
    "    \"\"\" Check that a and b values are equal.\n",
    "        \n",
    "        a: any value,\n",
    "        b: any value \"\"\"\n",
    "\n",
    "    return a==b\n",
    "\n",
    "def large_equality(a,b):\n",
    "\n",
    "    \"\"\" Check that years a and b expressed with four numbers are within the same decade.\n",
    "        \n",
    "        a: year,\n",
    "        b: year \"\"\"\n",
    "\n",
    "    return str(a)[:-1]==str(b)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comparison_vector(A_record, B, identifiers):\n",
    "    \n",
    "    \"\"\" Compare one record in A with all records in B. \n",
    "        Return the binary comparison of the identifiers for one record in A with all records in B.\n",
    "\n",
    "        A_record:     series of one row, \n",
    "        B:            dataframe, \n",
    "        identifiers:  dict: k = column name, \n",
    "                            v = method in {'large','strict','levenshtein','jaro-winkler'}\n",
    "    \"\"\"\n",
    "\n",
    "    methods = {'jaro-winkler':jaro_winkler_similarity, 'levenshtein':levenshtein_similarity, 'strict':strict_equality, 'large':large_equality}\n",
    "    comparisons = {}\n",
    "    for linking_var in identifiers:\n",
    "        method = methods[identifiers[linking_var]]\n",
    "        comparisons[linking_var] = np.array(B.apply(lambda row: method(A_record[linking_var], row[linking_var]), axis=1)).reshape(-1,1)\n",
    "    return np.concatenate(tuple(comparisons.values()), axis = 1) \n",
    "    \n",
    "A_record = A.iloc[40,:]\n",
    "comparison_vector(A_record, B, identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# match: probability of having same linking var when being true matches\n",
    "# unmatch: probability of having same linking var (at all)\n",
    "\n",
    "match = np.repeat(0.95, len(identifiers.keys()))\n",
    "unmatch = A.apply(lambda row: comparison_vector(row, B, identifiers).sum(axis=0), axis=1).sum() / (A.shape[0]*B.shape[0]) # for each A record and for each linking variable, we check 'the probability' to match (over the nA * nB pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " 'B': array([20, 21, 18, 25,  3,  1,  9, 13, 24,  5, 37, 26,  6, 12, 43, 33,  1,\n",
       "         6, 26, 12, 10, 48,  3, 48, 19, 28, 21, 24, 27, 22, 36, 32,  7, 15,\n",
       "        25, 31,  4,  2, 26, 36, 11, 23,  8, 25,  0, 11, 36, 37,  3, 20, 40,\n",
       "         7, 23, 43,  4, 31, 46, 47, 48, 49]),\n",
       " 'scores': array([11.90991281,  2.02253727, 12.90991281, 12.90991281, 11.90991281,\n",
       "         2.02253727, 12.90991281, 12.90991281, 12.90991281, -4.48586935,\n",
       "        12.90991281,  2.02253727,  2.02253727,  2.02253727, 12.90991281,\n",
       "        12.90991281, 12.90991281, 12.90991281, 12.90991281, 12.90991281,\n",
       "        12.90991281, 12.90991281,  2.02253727, -2.24077913, 12.90991281,\n",
       "        -2.24077913, 12.90991281,  2.02253727, -4.48586935, 12.90991281,\n",
       "         2.02253727, 12.90991281, 11.90991281, 12.90991281, -2.24077913,\n",
       "        11.90991281, 11.90991281, 12.90991281,  2.02253727, 12.90991281,\n",
       "        11.90991281, 11.90991281, 12.90991281, -2.24077913, 12.90991281,\n",
       "        11.90991281, 12.90991281, 12.90991281, 11.90991281, 11.90991281,\n",
       "        12.90991281, 11.90991281, 11.90991281, 12.90991281, 11.90991281,\n",
       "        11.90991281, 12.90991281, 12.90991281, 12.90991281, 12.90991281])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linking_score(A, B, identifiers, match, unmatch):\n",
    "        \n",
    "    \"\"\" Compare records in A with records in B, computing all linking scores for records in A with records in B. \n",
    "        Return the indices of records in A with the best match index for record in B.\n",
    "\n",
    "        A:            dataframe, \n",
    "        B:            dataframe, \n",
    "        identifiers:  dict: k = column name, \n",
    "                            v = method in {'large','strict','levenshtein','jaro-winkler'}\n",
    "        match:        array of probabilities of having same linking variables when being a match,\n",
    "        unmatch:      array of probabilities of having same linking variables (at all, among the nA x nB pairs of record).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_max_linking_score(A_record, B, identifiers, match, unmatch):\n",
    "        similarities = comparison_vector(A_record, B, identifiers)\n",
    "        if similarities.all(axis=1).sum() == 1: # there is one unique match: possible true match\n",
    "            return B.iloc[similarities.all(axis=1),:].index[0], np.NaN\n",
    "        else:\n",
    "            linking_score = (np.multiply(similarities, np.log2(match/unmatch)) + np.multiply(1-similarities, np.log2((1-match)/(1-unmatch)))).sum(axis=1)\n",
    "            return linking_score.argmax(), linking_score.max()\n",
    "\n",
    "    links = A.apply(lambda row: compute_max_linking_score(row[list(identifiers.keys())], B, identifiers, match, unmatch), axis=1)\n",
    "    idx_in_A = np.arange(A.shape[0])\n",
    "    idx_in_B = np.array([element[0] for element in links])\n",
    "    matching_scores = np.array([element[1] for element in links])\n",
    "    max_score = np.nanmax(matching_scores)\n",
    "    matching_scores = np.nan_to_num(matching_scores, nan=max_score+1) # ossible true matches are given a higher score\n",
    "    return {'A':idx_in_A, 'B':idx_in_B, 'scores':matching_scores}\n",
    "\n",
    "linking_score(A, B, identifiers, match, unmatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.294127882051875, 659.3656778153706)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stratified_ATE(DF_group, pop_size):\n",
    "\n",
    "    \"\"\" Compute the Average Treatment Effect on the specific stratum represented in DF_group.\n",
    "\n",
    "        DF_group: dataframe. \n",
    "    \"\"\"\n",
    "\n",
    "    n_treated, n_untreated = DF_group[DF_group.treatment == 1].shape[0], DF_group[DF_group.treatment == 0].shape[0]\n",
    "    assert (n_treated!=0)&(n_untreated!=0), (\"One group among treated/untreated is empty with this stratification.\")\n",
    "    avg_outcome_treated = (DF_group.treatment * DF_group.Y).sum() / n_treated \n",
    "    avg_outcome_untreated = ((1 - DF_group.treatment) * DF_group.Y).sum() / n_untreated\n",
    "    var_treated = ( (DF_group.treatment * DF_group.Y - avg_outcome_treated)**2 ).sum() / (n_treated - 1)\n",
    "    var_untreated = ( ((1 - DF_group.treatment) * DF_group.Y - avg_outcome_untreated)**2 ).sum() / (n_untreated - 1)\n",
    "    size_group = DF_group.shape[0]\n",
    "    ATE_group = (size_group/pop_size) * (avg_outcome_treated - avg_outcome_untreated)\n",
    "    variance_group = (size_group/pop_size)**2 * (var_untreated/n_untreated + var_treated/n_treated)\n",
    "    return ATE_group, variance_group\n",
    "\n",
    "stratified_ATE(DF, DF.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24562854, -0.62480721, -0.83119896, -0.34601315, -0.56723824,\n",
       "       -0.38506722, -0.42154433, -0.09940014, -0.75055998, -1.05732297,\n",
       "       -0.96563948, -0.69103408, -0.62744196, -0.69477582, -0.79408168,\n",
       "       -0.94897301, -1.06338381, -0.4150786 , -0.67903307, -0.67575906,\n",
       "       -0.86799301, -0.69891275, -0.60579753, -0.68251425, -0.83952037,\n",
       "       -0.54761415, -0.25637736, -0.42153576, -0.36500929, -0.56421259,\n",
       "       -1.09662174, -0.68028281, -0.19789087, -0.38843211, -0.58636426,\n",
       "        0.05849329, -0.68959717, -0.65863703, -0.77049132, -0.64023906,\n",
       "       -0.47072301, -0.1497882 , -0.88817401, -0.54896316, -0.33271651,\n",
       "       -0.04997487, -0.23080071, -0.56559951, -0.89318468, -0.37636724,\n",
       "       -0.11269199, -0.36144691, -0.25485061, -0.29643366, -0.41099934,\n",
       "       -0.59814537, -0.64360456, -0.46011144, -0.55477808, -0.4015365 ,\n",
       "       -0.23718291, -0.51811752, -0.65835297, -0.62057378, -0.90680327,\n",
       "       -0.14491584, -0.57845571, -0.68704653, -0.50665226, -0.17271989,\n",
       "       -0.59069539, -0.32595275, -0.56390479])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def propensity_score(DF, covariates, scaler, convert_to_logit):\n",
    "    \n",
    "    \"\"\" Compute propensity score estimates: the probability (logistic regression) that an observation is treated or not conditioned on some covariates.\n",
    "\n",
    "        DF:                dataframe,\n",
    "        covariates:        list of strings for covariates variable in DF,\n",
    "        scaler:            sklearn.preprocessing function scaler for exemple,\n",
    "        convert_to_logit:  boolean for converting probabilities to logit when building the propensity score estimates based on a logistic regression\n",
    "    \"\"\"\n",
    "\n",
    "    if scaler != None:\n",
    "        pipe = Pipeline([('scaler', scaler),('logistic_classifier', LogisticRegression())])\n",
    "    else:\n",
    "        pipe = Pipeline([('logistic_classifier', LogisticRegression())])\n",
    "    pipe.fit(DF[covariates], DF.treatment)\n",
    "    predictions = pipe.predict_proba(DF[covariates])\n",
    "    if convert_to_logit:\n",
    "        predictions_logit = logit(predictions[:,1])\n",
    "        return predictions_logit\n",
    "    else: \n",
    "        return predictions[:,1]\n",
    "\n",
    "propensity_score(DF, covariates, MinMaxScaler(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.206827174354896, 3387.381045181751)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ATE(DF, strata): # covariates=None, scaler=None, convert_to_logit=None,\n",
    "    \n",
    "    \"\"\" Compute the Average Treatment Effect in DF according to the stratification method:\n",
    "        no stratification when strata is None, \n",
    "        stratified dataframe build based on the list of specific covariates when one is passed,\n",
    "        or propensity score stratification.\n",
    "        Propensity score estimates are built conditionaly on covariates passed using a logistic regression after transformation by scaler (when one is specified).\n",
    "        Estimated probabilities can be converted into logit (convert_to_logit parameter).\n",
    "        Quantiles are used to partition the data based on propensity score estimates.\n",
    "        'covariates', 'scaler', 'convert_to_logit', 'quantiles' are only needed for 'propensity stratification' method.\n",
    "\n",
    "        DF:                dataframe,\n",
    "        strata:            value in {None, [...], 'propensity stratification'},\n",
    "        covariates:        list of strings for covariates variable in DF,\n",
    "        scaler:            sklearn.preprocessing function scaler for exemple,\n",
    "        convert_to_logit:  boolean for converting probabilities to logit when building the propensity score estimates based on a logistic regression,\n",
    "        quantiles:         list of quantiles (>0) to consider to build strata based on propensity score\n",
    "    \"\"\"\n",
    "    \n",
    "    pop_size = DF.shape[0]\n",
    "    if strata == None: # no stratification\n",
    "        return stratified_ATE(DF, pop_size)\n",
    "    elif strata == 'propensity stratification': # propensity score stratification\n",
    "        assert ('propensity_score' in DF.columns)&('prop_score_quantile' in DF.columns), (\"For propensity score stratification you need first to add the propensity_score and prop_score_quantile columns into the dataframe.\")\n",
    "        ATE, variance = 0, 0\n",
    "        for q in DF['prop_score_quantile'].unique():\n",
    "            stratum_data = DF[DF['prop_score_quantile'] == q]\n",
    "            ATE_stratum, variance_stratum = stratified_ATE(stratum_data, pop_size)\n",
    "            ATE += ATE_stratum\n",
    "            variance += variance_stratum\n",
    "        return ATE, variance\n",
    "    else: # stratification based on the covariates passed\n",
    "        ATE, variance = 0, 0\n",
    "        for stratum in DF.groupby(strata):\n",
    "            stratum_id, stratum_data = stratum\n",
    "            ATE_stratum, variance_stratum = stratified_ATE(stratum_data, pop_size)\n",
    "            ATE += ATE_stratum\n",
    "            variance += variance_stratum\n",
    "        return ATE, variance\n",
    "\n",
    "ATE(DF, 'propensity stratification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.294127882051875, 659.3656778153706)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATE(DF, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/32t15g3d2h54d0ct6cssb9gh0000gn/T/ipykernel_31775/3199416953.py:34: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for stratum in DF.groupby(strata):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.76974768988275, 706.732593652973)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATE(DF, ['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([47.33719513914916,\n",
       "  46.39276952192214,\n",
       "  46.153054900522065,\n",
       "  45.27140935392173,\n",
       "  44.72825817031678,\n",
       "  15.600965590480033,\n",
       "  15.49195327496395,\n",
       "  15.309637357517015,\n",
       "  13.781570614273592,\n",
       "  13.781329335788602],\n",
       " [69.30381736813325,\n",
       "  65.3316786061727,\n",
       "  67.11109893024654,\n",
       "  64.5648768468791,\n",
       "  61.89730849861741,\n",
       "  13.821247094960613,\n",
       "  13.805832166752769,\n",
       "  13.432986717798697,\n",
       "  11.510034145134389,\n",
       "  11.402201028704862])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Estimate_Tethered_Stopping_Rule(A, B, identifiers, match, unmatch, covariates, scaler, convert_to_logit):\n",
    "\n",
    "    \"\"\" Compare records in A with records in B, computing all linking scores for records in A with records in B. \n",
    "        Return the indices of records in A with the best match index for record in B.\n",
    "\n",
    "        A: dataframe, \n",
    "        B: dataframe, \n",
    "        identifiers: dict: k = column name, v = method in {'large','strict','levenshtein','jaro-winkler',\n",
    "        match: array of probabilities of having same linking variables when being a match,\n",
    "        unmatch: array of probabilities of having same linking variables (at all, among the nA x nB pairs of record),\n",
    "        strata: .\n",
    "    \"\"\"\n",
    "    \n",
    "    matchings = linking_score(A, B, identifiers, match, unmatch)\n",
    "    for score in np.sort(np.unique(matchings['scores']))[::-1]:\n",
    "        best_matches = matchings['A'][matchings['scores'] == score]\n",
    "        from_A = A.iloc[best_matches,:].drop('source', axis=1).reset_index(drop=True)\n",
    "        from_B = B.iloc[matchings['B'][best_matches],:]['Y'].reset_index(drop=True)\n",
    "        linked_records = pds.concat([linked_records, pds.concat([from_A,from_B], axis = 1)])\n",
    "        ATE_links, variance_links = ATE(linked_records, 'propensity stratification')\n",
    "        ATE_list.append(ATE_links)\n",
    "        Var_list.append(variance_links)\n",
    "    return ATE_list, Var_list\n",
    "\n",
    "A['propensity_score'] = propensity_score(A, covariates, MinMaxScaler(), True)\n",
    "q = 5 # recommended\n",
    "A['prop_score_quantile'] = pds.qcut(A['propensity_score'], q, labels = False)\n",
    "\n",
    "# check the balance\n",
    "np.array([(A[A['prop_score_quantile']==value].treatment.value_counts() > A.shape[0]/(q*3)).all() for value in range(q)]).all()\n",
    "\n",
    "Estimate_Tethered_Stopping_Rule(A, B, identifiers, match, unmatch, ['X1','X2','X3'], MinMaxScaler(), True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b95706f4894ee4355943f31879e0e3a76d825289e1bc7e0d16bc1dcbc4ba3b09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
